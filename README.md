# Autonomous Highway Simulation System

## **Project Overview**
This project is an advanced **autonomous highway simulation environment** designed to model and evaluate autonomous driving behaviors, particularly focused on lane-changing, velocity control, and safe navigation in highway scenarios. The system simulates real-world scenarios, allowing for testing of decision-making algorithms, trajectory planning, and dynamic vehicle control.

The aim of the project is to **develop and simulate an ego vehicle that can navigate highways autonomously**, adhering to traffic rules, maintaining high speeds within the speed limit, and performing safe overtaking maneuvers. It integrates reinforcement learning, trajectory generation, and a modular simulation environment for autonomous driving research.

---

## **Project Structure and Subsystems**
The system is divided into several key subsystems, each with a specific role in the overall simulation:

### 1. **Reinforcement Learning Model**
- **Description:** The reinforcement learning (RL) subsystem is responsible for **training and controlling the ego vehicle** to make optimal decisions regarding lane-changing, speed control, and collision avoidance. The model interacts with the environment by receiving observations and outputting control actions (e.g., accelerate, decelerate, change lane).

- **Key Features:**
  - Observation space includes positions, velocities, and relative distances of nearby vehicles.
  - Action space defines discrete control options (e.g., lane changes, acceleration levels).
  - Reward functions balance speed maximization, lane discipline, and safety.
  - Supports training using popular RL libraries (e.g., Stable-Baselines3).

### 2. **Trajectory Planner**
- **Description:** The trajectory planner generates smooth and safe paths for the ego vehicle to follow based on its current state and desired maneuvers using quintic polynomials. It accounts for acceleration, jerk constraints, and vehicle dynamics to ensure passenger comfort and safety.

- **Key Features:**
  - Computes feasible trajectories for lane changes.
  - Evaluates trajectory quality based on acceleration and jerk limits.
  - Interfaces with the RL model or rule-based decision-making to generate optimal paths.
  - Assesses the planned trajectory for violations or unsafe maneuvers.
  Planned trajectories for lane changes look like this
![Screenshot from 2025-02-05 01-11-40](https://github.com/user-attachments/assets/c5964ce0-84d9-4de3-8912-0f530088e816)

### 3. **Trajectory Controller**
- **Description:** The trajectory controller is responsible for **executing the planned trajectories** by converting them into low-level control commands for the ego vehicle. It uses a **model predictive control (MPC)** module to compute the steering angle, throttle, and brake values needed to follow the trajectory while maintaining stability and handling external disturbances.

- **Key Features:**
  - Follows the trajectory by computing appropriate control signals using MPC.
  - Maintains stability and adjusts dynamically if the trajectory needs corrections.
  - Calculates optimal steering and throttle values to minimize trajectory deviation and ensure smooth vehicle control.
  - Follows the trajectory by computing appropriate control signals.
  - Maintains stability and adjusts dynamically if the trajectory needs corrections.
## **Demonstration of Video Results**

To showcase the performance of the system, video demonstrations of simulation runs are available. These videos highlight the behavior of the ego vehicle as it navigates the highway, performs lane changes, and reacts to surrounding traffic conditions.

### **Key Demonstrations:**
- **Safe Overtaking:** The ego vehicle successfully overtakes slower vehicles using optimal lane changes.
- **Speed Management:** Demonstrates how the system maintains high speeds within safe limits while avoiding collisions.
- **Trajectory Tracking:** Shows the smooth transitions generated by the trajectory planner and executed by the trajectory tracker.
- **Traffic Interactions:** Highlights realistic interactions between the ego vehicle and simulated traffic using MOBIL and IDM.

### **Demonstration Scenarios:**

- Action space: is defined with this
```python
- Enum class Action(Enum):
    NO_ACTION = 0
    CHANGE_LANE_RIGHT = 1
    CHANGE_LANE_LEFT = 2
    ACCELERATE = 3
    DECELERATE = 4
    EMERGENCY_BRAKE = 5
 ```
We demonstrate the performance of two models tested under different traffic scenarios:

1. **Model 1 (Less Aggressive):**
   - **Description:** This model prioritizes safety and lane discipline, making conservative lane changes and speed adjustments.
   - **Scenario:** Tested in a dense traffic environment with many surrounding vehicles.
   - **Highlights:** Shows how the system handles complex traffic conditions, avoids collisions, and maintains smooth driving.
   - **Reward Shaping:** Given reward for driving 100-130 km/h, not driving on the left most lane and given punishment for low ttc driving.
   - **Most important remark:** Reinforcement learning agent learned not to overtake from right without explicitly told.


https://github.com/user-attachments/assets/0176f439-30ed-4560-ae47-14ffc35e7f8b



2. **Model 2 (More Aggressive):**
   - **Description:** This model prioritizes speed and fast overtaking by making more assertive lane changes.
   - **Scenario:** Tested in a scenario with fewer vehicles on the road, allowing for more aggressive maneuvers.
   - **Highlights:** Demonstrates efficient overtaking, high-speed driving, and optimal use of available lanes.
  - **Reward Shaping:** Given reward for driving 130-150 km/h, given punishment for low ttc driving. Less punishment compared to first model.
    
https://github.com/user-attachments/assets/e538e2df-4a40-4265-af37-1c0ef72deeb1

For further details of the visualization on the right, check out [Reinforcement Learning action probability visualizer](https://github.com/bdrhnsen/rl_visualizer).


---


