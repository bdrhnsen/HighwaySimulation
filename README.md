# Highway Simulation

## **Project Overview**
This project is an advanced **highway simulation environment** designed to model and evaluate autonomous driving behaviors, particularly focused on lane-changing, velocity control, and safe navigation in highway scenarios. The system simulates real-world scenarios, allowing for testing of decision-making algorithms, trajectory planning, and dynamic vehicle control.

The aim of the project is to **develop and simulate an ego vehicle that can navigate highways autonomously**, adhering to traffic rules, maintaining high speeds within the speed limit, and performing safe overtaking maneuvers. It integrates reinforcement learning, trajectory generation, and a modular simulation environment for autonomous driving research.

---

## **Project Structure and Subsystems**
The system is divided into several key subsystems, each with a specific role in the overall simulation:

### 1. **Reinforcement Learning Model**
- **Description:** The reinforcement learning (RL) subsystem is responsible for **training and controlling the ego vehicle** to make optimal decisions regarding lane-changing, speed control, and collision avoidance. The model interacts with the environment by receiving observations and outputting control actions (e.g., accelerate, decelerate, change lane).

- **Key Features:**
  - Observation space includes positions, velocities, and relative distances of nearby vehicles.
  - Action space defines discrete control options (e.g., lane changes, acceleration levels).
  - Reward functions balance speed maximization, lane discipline, and safety.
  - Supports training using popular RL libraries (e.g., Stable-Baselines3).

### 2. **Trajectory Planner**
- **Description:** The trajectory planner generates smooth and safe paths for the ego vehicle to follow based on its current state and desired maneuvers using quintic polynomials. It accounts for acceleration, jerk constraints, and vehicle dynamics to ensure passenger comfort and safety.

- **Key Features:**
  - Computes feasible trajectories for lane changes.
  - Evaluates trajectory quality based on acceleration and jerk limits.
  - Interfaces with the RL model or rule-based decision-making to generate optimal paths.
  - Assesses the planned trajectory for violations or unsafe maneuvers.
  Planned trajectories for lane changes look like this
![Screenshot from 2025-02-05 01-11-40](https://github.com/user-attachments/assets/c5964ce0-84d9-4de3-8912-0f530088e816)

### 3. **Trajectory Controller**
- **Description:** The trajectory controller is responsible for **executing the planned trajectories** by converting them into low-level control commands for the ego vehicle. It uses a **model predictive control (MPC)** module to compute the steering angle, throttle, and brake values needed to follow the trajectory while maintaining stability and handling external disturbances.

- **Key Features:**
  - Follows the trajectory by computing appropriate control signals using MPC.
  - Maintains stability and adjusts dynamically if the trajectory needs corrections.
  - Calculates optimal steering and throttle values to minimize trajectory deviation and ensure smooth vehicle control.
  - Follows the trajectory by computing appropriate control signals.
  - Maintains stability and adjusts dynamically if the trajectory needs corrections.
## **Demonstration of Video Results**

To showcase the performance of the system, video demonstrations of simulation runs are available. These videos highlight the behavior of the ego vehicle as it navigates the highway, performs lane changes, and reacts to surrounding traffic conditions.

### **Key Demonstrations:**
- **Safe Overtaking:** The ego vehicle successfully overtakes slower vehicles using optimal lane changes.
- **Speed Management:** Demonstrates how the system maintains high speeds within safe limits while avoiding collisions.
- **Trajectory Tracking:** Shows the smooth transitions generated by the trajectory planner and executed by the trajectory tracker.
- **Traffic Interactions:** Highlights realistic interactions between the ego vehicle and simulated traffic using MOBIL and IDM.

### **Demonstration Scenarios:**

- Action space: is defined with this
```python
- Enum class Action(Enum):
    NO_ACTION = 0
    CHANGE_LANE_RIGHT = 1
    CHANGE_LANE_LEFT = 2
    ACCELERATE = 3
    DECELERATE = 4
    EMERGENCY_BRAKE = 5
 ```
We demonstrate the performance of two models tested under different traffic scenarios:

1. **Model 1 (Less Aggressive):**
   - **Description:** This model prioritizes safety and lane discipline, making conservative lane changes and speed adjustments.
   - **Scenario:** Tested in a dense traffic environment with many surrounding vehicles.
   - **Highlights:** Shows how the system handles complex traffic conditions, avoids collisions, and maintains smooth driving.
   - **Reward Shaping:** Given reward for driving 100-130 km/h, not driving on the left most lane and given punishment for low ttc driving.
   - **Most important remark:** Reinforcement learning agent learned not to overtake from right without explicitly told.


https://github.com/user-attachments/assets/0176f439-30ed-4560-ae47-14ffc35e7f8b



2. **Model 2 (More Aggressive):**
   - **Description:** This model prioritizes speed and fast overtaking by making more assertive lane changes.
   - **Scenario:** Tested in a scenario with fewer vehicles on the road, allowing for more aggressive maneuvers.
   - **Highlights:** Demonstrates efficient overtaking, high-speed driving, and optimal use of available lanes.
  - **Reward Shaping:** Given reward for driving 130-150 km/h, given punishment for low ttc driving. Less punishment compared to first model.
    
https://github.com/user-attachments/assets/e538e2df-4a40-4265-af37-1c0ef72deeb1

For further details of the visualization on the right, check out [Reinforcement Learning action probability visualizer](https://github.com/bdrhnsen/rl_visualizer).


---
## Installation

Clone the repository and install the required dependencies:

```bash
# Clone the repository
# create a virtualenv and install requirements
python -m venv env && source /env/bin/activate && pip install requirements.txt

# Install the package
cd highway_simulation
pip install -e .

# Run the trained agent
 cd .. && python main.py

```


## Getting Started

### 1. Environment Setup

The primary environment, `HighwayEnv`, is registered under Gymnasium. Here's how to create and test the environment:

```python
import gymnasium as gym
from highway_simulation.environments.relative_to_ego_highway_env import HighwayEnv

# Initialize the environment
env = gym.make("highway_env")

# Test the environment
obs = env.reset()
for _ in range(100):
    action = env.action_space.sample()  # Take random actions
    obs, reward, done, _, _ = env.step(action)
    env.render()

env.close()
```

### 2. Training an RL Agent

Train a policy using **Stable-Baselines3**:

```python
from stable_baselines3 import PPO
from highway_simulation.environments.relative_to_ego_highway_env import HighwayEnv

# Initialize the environment
env = gym.make("highway_env")

# Train PPO policy
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100000)

# Save the model
model.save("ppo_highway_model")
```

### 3. Testing and Visualization

Evaluate trained policies using predefined test cases:

```python
import highway_simulation
import gymnasium as gym

from stable_baselines3 import PPO

env = gym.make(id='highway_env')
model = PPO.load("./PPO_MODEL/", env=env) 

def main():
    while True:
        env.seed(2)
        obs,_ = env.reset()
        done = False

        while not done:
            action, state = model.predict(obs, deterministic=True) # RL chooses action based on the trained model
            obs, reward, done, _,_ = env.step(action) # applying the action
            env.render() # Rendering the environment with pygame
            if done:
                obs = env.reset()

        env.close()


if __name__ == "__main__":
    main()
```

---

## Code Structure

### Project Layout

```plaintext
highway_simulation/
├── environments/       # Highway environment definitions
├── scripts/            # Core simulation logic (vehicles, lanes, rewards, etc.)
├── testing/            # Test scripts for RL policies
├── tests/              # Unit tests for simulation components
```

### Key Components

1. **Environment (`environments/`)**:

   - `relative_to_ego_highway_env.py`: Main simulation environment.
   - Configurable action and observation spaces.

2. **Simulation Core (`scripts/`)**:

   - `highway.py`: Manages the overall simulation.
   - `vehicle.py`: Handles vehicle dynamics and properties. Longitudinally, vehicles are driven by IDM
   - `laneManager.py`: Manages lane and traffic dynamics.
   - `rewardCalculator.py`: Manages calculating rewards based on the states and actions.

3. **Planning (`scripts/planning/`)**:

   - `trajectory_planner.py`: Generates smooth trajectories with quintic polynomial trajectories.
   - `decision_to_trajectory.py`: Converts decisions to trajectories.



## Example Configurations

Adjust environment parameters via the `Config` object:

```python
from highway_simulation.scripts.config import Config

config = Config(
                min_vel=13,max_vel=36,
                min_rewardable_vel=21, max_rewardable_vel=29,
                collision_threshold=2,lane_change_duration=6,
                num_of_vehicles=35, road_length=51000,
                vehicle_width=4.5,
                vehicle_height=2,
                screen_width=1499,
                screen_height=1000,
                lane_width=3.5,
                time_step=0.3,
                num_lanes=3,
                effective_sim_length=20000,
                effective_sim_time=120)
```



## Contact

For inquiries or support, contact the author:

- **Name**: Bedirhan Sen
- **Email**: [bdrhnsen@gmail.com](mailto:bdrhnsen@gmail.com)
- **GitHub**: [bdrhnsen](https://github.com/bdrhnsen)
- **LinkedIn**: [bedirhan-sen](https://www.linkedin.com/in/bedirhan-sen/)






